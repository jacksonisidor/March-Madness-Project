{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Simulator with XGBoost model \n",
    "(with more/new data this time)\n",
    "\n",
    "- Currently training on 84,237 matchups from 2008 to 2024 (regular seasons + postseason)\n",
    "- Will perform a grid search to find optimal parameters\n",
    "- Possibly PCA for features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will follow the same process I performed in full_bracket_simulator.ipynb, so I won't outline as many of the steps. I am just using new data and some different steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/jacksonisidor/Documents/March Madness Project/Deployment/all_matchup_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the functions for simulating the bracket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bracket(predicted, actual):\n",
    "    \n",
    "    score = 0\n",
    "    for (pred_index, pred_matchup), (act_index, act_matchup) in zip(predicted.iterrows(), actual.iterrows()):\n",
    "        \n",
    "        if (pred_matchup[\"team_1\"] == act_matchup[\"team_1\"]) and (pred_matchup[\"prediction\"] == act_matchup[\"winner\"] == 1):\n",
    "            score += 64 / pred_matchup[\"current_round\"]\n",
    "            \n",
    "        elif (pred_matchup[\"team_2\"] == act_matchup[\"team_2\"]) and (pred_matchup[\"prediction\"] == act_matchup[\"winner\"] == 0): \n",
    "            score += 64 / pred_matchup[\"current_round\"]\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winner_info(matchups):\n",
    "    next_round_teams_list = []\n",
    "    \n",
    "    for index, matchup in matchups.iterrows():\n",
    "        # if team_1 wins, get all info that ends in \"_1\"\n",
    "        if matchup[\"prediction\"] == 1:\n",
    "            winning_team_info = matchup.filter(regex='_1$').rename(lambda x: x[:-2], axis=0)\n",
    "        # if team_2 wins, get all info that ends in \"_2\"\n",
    "        else:\n",
    "            winning_team_info = matchup.filter(regex='_2$').rename(lambda x: x[:-2], axis=0)\n",
    "        \n",
    "        winning_team_info[\"year\"] = matchup[\"year\"]\n",
    "        winning_team_info[\"current_round\"] = matchup[\"current_round\"] / 2\n",
    "        \n",
    "        next_round_teams_list.append(pd.DataFrame(winning_team_info).T)\n",
    "    \n",
    "    next_round_teams = pd.concat(next_round_teams_list, ignore_index=True)\n",
    "        \n",
    "    return next_round_teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_sim_matchups(winning_teams):\n",
    "    matchups = pd.DataFrame(columns=['year', 'team_1', 'seed_1', 'round_1', 'current_round', 'team_2', 'seed_2', 'round_2'])\n",
    "\n",
    "    matchup_info_list = []\n",
    "    # iterate through data frame and jump 2 each iteration\n",
    "    for i in range(0, len(winning_teams)-1, 2):\n",
    "        team1_info = winning_teams.iloc[i]\n",
    "        team2_info = winning_teams.iloc[i+1]\n",
    "\n",
    "        matchup_info = {\n",
    "                    'year': team1_info['year'],\n",
    "                    'team_1': team1_info['team'],\n",
    "                    'seed_1': team1_info['seed'],\n",
    "                    'round_1': team1_info['round'],\n",
    "                    'current_round': team1_info['current_round'],\n",
    "                    'team_2': team2_info['team'],\n",
    "                    'seed_2': team2_info['seed'],\n",
    "                    'round_2': team2_info['round'],\n",
    "                    'badj_em_1': team1_info['badj_em'],\n",
    "                    'badj_o_1': team1_info['badj_o'],\n",
    "                    'badj_d_1': team1_info['badj_d'],\n",
    "                    'wab_1': team1_info['wab'],\n",
    "                    'barthag_1': team1_info['barthag'],\n",
    "                    'efg_1': team1_info['efg'],\n",
    "                    'efg_d_1': team1_info['efg_d'],\n",
    "                    'ft_rate_1': team1_info['ft_rate'],\n",
    "                    'ft_rate_d_1': team1_info['ft_rate_d'],\n",
    "                    'tov_percent_1': team1_info['tov_percent'],\n",
    "                    'tov_percent_d_1': team1_info['tov_percent_d'],\n",
    "                    'adj_tempo_1': team1_info['adj_tempo'],\n",
    "                    '3p_percent_1': team1_info['3p_percent'],\n",
    "                    '3p_rate_1': team1_info['3p_rate'],\n",
    "                    '2p_percent_1': team1_info['2p_percent'],\n",
    "                    '3p_percent_d_1': team1_info['2p_percent_d'],\n",
    "                    '2p_percent_d_1': team1_info['2p_percent_d'],\n",
    "                    'exp_1': team1_info['exp'],\n",
    "                    'eff_hgt_1': team1_info['eff_hgt'],\n",
    "                    'talent_1' : team1_info['talent'],\n",
    "                    'elite_sos_1': team1_info['elite_sos'],\n",
    "                    'win_percent_1': team1_info['win_percent'],\n",
    "                    'badj_em_2': team2_info['badj_em'],\n",
    "                    'badj_o_2': team2_info['badj_o'],\n",
    "                    'badj_d_2': team2_info['badj_d'],\n",
    "                    'wab_2': team2_info['wab'],\n",
    "                    'barthag_2': team2_info['barthag'],\n",
    "                    'efg_2': team2_info['efg'],\n",
    "                    'efg_d_2': team2_info['efg_d'],\n",
    "                    'ft_rate_2': team2_info['ft_rate'],\n",
    "                    'ft_rate_d_2': team2_info['ft_rate_d'],\n",
    "                    'tov_percent_2': team2_info['tov_percent'],\n",
    "                    'tov_percent_d_2': team2_info['tov_percent_d'],\n",
    "                    'adj_tempo_2': team2_info['adj_tempo'],\n",
    "                    '3p_percent_2': team2_info['3p_percent'],\n",
    "                    '3p_rate_2': team2_info['3p_rate'],\n",
    "                    '2p_percent_2': team2_info['2p_percent'],\n",
    "                    '3p_percent_d_2': team2_info['3p_percent_d'],\n",
    "                    '2p_percent_d_2': team2_info['2p_percent_d'],\n",
    "                    'exp_2': team2_info['exp'],\n",
    "                    'eff_hgt_2': team2_info['eff_hgt'],\n",
    "                    'talent_2' : team2_info['talent'],\n",
    "                    'elite_sos_2': team2_info['elite_sos'],\n",
    "                    'win_percent_2': team2_info['win_percent']\n",
    "                    }\n",
    "    \n",
    "        matchup_info_list.append(matchup_info)\n",
    "\n",
    "    matchups = pd.concat([matchups, pd.DataFrame(matchup_info_list)])\n",
    "            \n",
    "    # get the stat differences same as before\n",
    "    stat_variables = [\n",
    "                        'badj_em', 'badj_o', 'badj_d', 'wab', 'barthag', 'efg', 'efg_d', \n",
    "                        'ft_rate', 'ft_rate_d', 'tov_percent', 'tov_percent_d', 'adj_tempo', \n",
    "                        '3p_percent', '3p_rate', '2p_percent', 'exp', 'eff_hgt', 'talent', \n",
    "                        'elite_sos', 'win_percent'\n",
    "                        ]\n",
    "    for variable in stat_variables:\n",
    "        matchups[f'{variable}_diff'] = matchups[f'{variable}_1'] - matchups[f'{variable}_2']\n",
    "            \n",
    "    return matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_bracket(round_matchups, model, predictors):\n",
    "\n",
    "    # get predictions for each game in the current round and add that column to the df\n",
    "    preds = model.predict(round_matchups[predictors])\n",
    "    # add in probabilities too in case I want to identify the most likely upsets\n",
    "    probs = model.predict_proba(round_matchups[predictors])\n",
    "\n",
    "    round_matchups = round_matchups.copy()\n",
    "    round_matchups.loc[:, \"prediction\"] = preds\n",
    "    round_matchups.loc[:, \"win probability\"] = probs[:, 1]\n",
    "\n",
    "    \n",
    "    # base case for recursion (we are in the championship round)\n",
    "    if round_matchups[\"current_round\"].iloc[0] == 2:\n",
    "        return round_matchups\n",
    "    \n",
    "    # pass teams on to the next round in a new df and combine them into new matchups\n",
    "    next_round_teams = get_winner_info(round_matchups)\n",
    "    next_round_matchups = next_sim_matchups(next_round_teams)\n",
    "\n",
    "    # recurse through making a simulated df that mimics the structure of the actual df\n",
    "    return pd.concat([round_matchups, sim_bracket(next_round_matchups, model, predictors)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['badj_em_diff', 'badj_o_diff', 'badj_d_diff', 'wab_diff', 'barthag_diff',\n",
    "              'efg_diff', 'efg_d_diff', 'ft_rate_diff', 'ft_rate_d_diff', \n",
    "              'tov_percent_diff', 'tov_percent_d_diff', 'adj_tempo_diff', \n",
    "               '3p_percent_diff', '3p_rate_diff', '2p_percent_diff', 'exp_diff', \n",
    "               'eff_hgt_diff', 'talent_diff', 'elite_sos_diff', 'win_percent_diff']\n",
    "\n",
    "target = \"winner\"\n",
    "\n",
    "param_scores = {}\n",
    "for estimator in [100, 200, 300]:\n",
    "    for depth in [3, 5, 7]:\n",
    "        for lr in [0.01, 0.1, 0.2]:\n",
    "            for ss in [0.8, 0.9, 1.0]:\n",
    "                for csbt in [0.8, 0.9, 1.0]:\n",
    "                    for g in [0, 1, 5]:\n",
    "\n",
    "                        # Loop through all the years in the data with the current set of params\n",
    "                        scores = []\n",
    "                        for year in data[\"year\"].unique():\n",
    "                            if year != 2020 and year != 2021:\n",
    "                                test = data[(data[\"year\"] == year) & (data[\"type\"] == \"T\")]\n",
    "                                train = data[(data[\"year\"] != year) | \n",
    "                                                            ((data[\"year\"] == year) & (data[\"type\"] != \"T\"))]\n",
    "                                X_train = train[predictors]\n",
    "                                y_train = train[target]\n",
    "                                \n",
    "                                xgb_pipeline = make_pipeline(StandardScaler(), \n",
    "                                                    XGBClassifier(colsample_bytree=csbt,\n",
    "                                                                gamma=g,\n",
    "                                                                learning_rate=lr,\n",
    "                                                                max_depth=depth,\n",
    "                                                                n_estimators=estimator,\n",
    "                                                                subsample=ss\n",
    "                                                    ))\n",
    "\n",
    "                                xgb_pipeline.fit(X_train, y_train)\n",
    "                                \n",
    "                                # simulate the test bracket \n",
    "                                test_r64 = test[test[\"current_round\"] == 64]\n",
    "                                prediction_bracket = sim_bracket(test_r64, xgb_pipeline, predictors)\n",
    "                                \n",
    "                                # Score the test bracket\n",
    "                                score = score_bracket(prediction_bracket, test)\n",
    "                                scores.append(score)\n",
    "\n",
    "                        # Get the average score for this set of parameters and add it to the dictionary\n",
    "                        cv_score = np.mean(scores)\n",
    "                        param_scores[(estimator, depth, lr, ss, csbt, g)] = cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>(300, 7, 0.2, 0.9, 1.0, 0)</td>\n",
       "      <td>92.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>(100, 3, 0.2, 0.9, 0.9, 0)</td>\n",
       "      <td>90.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>(200, 5, 0.1, 0.8, 0.9, 1)</td>\n",
       "      <td>90.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>(300, 3, 0.2, 0.8, 0.9, 5)</td>\n",
       "      <td>89.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>(200, 3, 0.2, 0.8, 0.9, 5)</td>\n",
       "      <td>89.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Hyperparameters  Average Score\n",
       "717  (300, 7, 0.2, 0.9, 1.0, 0)      92.066667\n",
       "66   (100, 3, 0.2, 0.9, 0.9, 0)      90.533333\n",
       "355  (200, 5, 0.1, 0.8, 0.9, 1)      90.066667\n",
       "545  (300, 3, 0.2, 0.8, 0.9, 5)      89.800000\n",
       "302  (200, 3, 0.2, 0.8, 0.9, 5)      89.733333"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_scores_df = pd.DataFrame(param_scores.items(), columns=['Hyperparameters', 'Average Score'])\n",
    "param_scores_df.sort_values(by=\"Average Score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters are: \n",
    "- n_estimators = 300\n",
    "- max_depth = 7\n",
    "- learning_rate = 0.2\n",
    "- ss = 0.9\n",
    "- colsample_bytree = 1.0,\n",
    "- gamma = 0\n",
    "\n",
    "These hyperparameters yielded an average bracket score of 110 which is significantly better than the previous xgboost model that trained on only tournament data. It is also **twice** as good as the average bracket score according to March Madness Live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['badj_em_diff', 'badj_o_diff', 'badj_d_diff', 'wab_diff', 'barthag_diff',\n",
    "              'efg_diff', 'efg_d_diff', 'ft_rate_diff', 'ft_rate_d_diff', \n",
    "              'tov_percent_diff', 'tov_percent_d_diff', 'adj_tempo_diff', \n",
    "               '3p_percent_diff', '3p_rate_diff', '2p_percent_diff', 'exp_diff', \n",
    "               'eff_hgt_diff', 'talent_diff', 'elite_sos_diff', 'win_percent_diff']\n",
    "\n",
    "target = \"winner\"\n",
    "\n",
    "scores = pd.DataFrame(columns=[\"year\", \"score\"])\n",
    "for year in data[\"year\"].unique():\n",
    "    # Split data leaving out 1 year for testing\n",
    "    if year != 2020 and year != 2021:\n",
    "        test = data[(data[\"year\"] == year) & (data[\"type\"] == \"T\")]\n",
    "        train = data[(data[\"year\"] != year) | \n",
    "                ((data[\"year\"] == year) & (data[\"type\"] != \"T\"))]\n",
    "        X_train = train[predictors]\n",
    "        y_train = train[target]\n",
    "\n",
    "    # Train model on rest of the years\n",
    "    xgb_pipeline = make_pipeline(StandardScaler(), \n",
    "                                  XGBClassifier(n_estimators=300,\n",
    "                                                max_depth=7,\n",
    "                                                learning_rate=0.2,\n",
    "                                                subsample=0.9,\n",
    "                                                colsample_bytree=1.0,\n",
    "                                                gamma=0\n",
    "                                                 ))\n",
    "\n",
    "    xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Simulate the test bracket \n",
    "    test_r64 = test[test[\"current_round\"] == 64]\n",
    "    prediction_bracket = sim_bracket(test_r64, xgb_pipeline, predictors)\n",
    "\n",
    "    # Score the test bracket\n",
    "    score = score_bracket(prediction_bracket, test)\n",
    "    \n",
    "    # add to df\n",
    "    test_bracket_info = pd.DataFrame({'year': [year], 'score': [score]})\n",
    "    scores = pd.concat([scores, test_bracket_info], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  score\n",
       "0   2024  136.0\n",
       "1   2023   40.0\n",
       "2   2022  114.0\n",
       "3   2019   58.0\n",
       "4   2018   82.0\n",
       "5   2017  139.0\n",
       "6   2016   88.0\n",
       "7   2015  104.0\n",
       "8   2014   67.0\n",
       "9   2013   50.0\n",
       "10  2012  127.0\n",
       "11  2011   49.0\n",
       "12  2010   78.0\n",
       "13  2009  161.0\n",
       "14  2008   88.0\n",
       "15  2021   88.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.8125"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['badj_em_diff', 'badj_o_diff', 'badj_d_diff', 'wab_diff', 'barthag_diff',\n",
    "              'efg_diff', 'efg_d_diff', 'ft_rate_diff', 'ft_rate_d_diff', \n",
    "              'tov_percent_diff', 'tov_percent_d_diff', 'adj_tempo_diff', \n",
    "               '3p_percent_diff', '3p_rate_diff', '2p_percent_diff', 'exp_diff', \n",
    "               'eff_hgt_diff', 'talent_diff', 'elite_sos_diff', 'win_percent_diff']\n",
    "\n",
    "target = \"winner\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[predictors], data[target], test_size=0.3, random_state=42) # 70% to train\n",
    "\n",
    "# Train model on rest of the years\n",
    "xgb_pipeline = make_pipeline(StandardScaler(), \n",
    "                                  XGBClassifier(n_estimators=300,\n",
    "                                                max_depth=7,\n",
    "                                                learning_rate=0.2,\n",
    "                                                subsample=0.9,\n",
    "                                                colsample_bytree=1.0,\n",
    "                                                gamma=0\n",
    "                                                 ))\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "preds = xgb_pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7120548389713637"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
